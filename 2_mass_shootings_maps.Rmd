---
title: "Mass Shooting Gun Violence in the United States of America"
subtitle: "Maps and Geographic Analysis"
author: "Trevor Schrotz"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: lumen
    df_print: paged
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float: true
editor_options:
  chunk_output_type: inline
---

<style type="text/css">
.main-container {
  max-width: 1400px;
  margin-left: auto;
  margin-right: auto;
}
</style>

# Introduction

## Preface

This is part two in a look at gun violence mass shootings in the US.  In this analysis, we will look at the shooting incident data geographically.

# Setup

## Load Libraries

```{r, include=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidymodels)
library(tidytext)
library(lubridate)
library(scales)
library(janitor)
library(skimr)
library(tidyquant)
library(broom)
library(purrr)
library(rvest)
library(tidycensus)
library(tidygeocoder)
library(leaflet)
library(reticulate)
pd <- py_install("pandas")
api_key <- Sys.getenv("CENSUS_API_KEY")
```

# Source the data

Read in the saved data file.

```{r, include=TRUE}
complete_tbl <- read_rds("gva_data.rds") 
```

Here I prepare the primary data set to include some date-related fields for convenience when plotting and summarizing the data.

```{r cleaning, include=TRUE}
ms_tbl <- complete_tbl %>% 
    janitor::clean_names() %>% 
    select(-operations) %>% 
    mutate(incident_date = mdy(incident_date),
           year = year(incident_date),
           month = month(incident_date, label = T, abbr = T),
           incident_month = rollforward(incident_date),
           dow = wday(incident_date, label = TRUE)) %>% 
    arrange(incident_date)
```

# Pull the incident IDs from the GVA data which then become part of the URL for the incident-specific information.

```{r}
incident_ids <- ms_tbl %>%
    distinct(incident_id) # get a list of all of the incident IDs

geo_locations_incidents_tbl <- incident_ids %>%
    mutate(pages = str_glue("https://www.gunviolencearchive.org/incident/{incident_id}")) # create a tibble with all of the URLs needed
```

The initial reason I started this project was to work on a web scraping endeavor.  What I began with was part one of this project, and scraping the initial mass shooting data was very straightforward, and I learned a great deal about how to accomplish that process in R.  However, as things progressed and I wanted to mine more of the incident data, I discovered that the GVA website has a robot algorithm in place to verify that a browser is likely a human and not a bot.  There were several helpful directives online regarding using selenium as a headless browser, which I started doing in using RSelenium.  The website still recognized that I was using a controlled browser, and it denied my access.  Eventually, I needed to adjust some of the Chrome options, and I was not able to make that happen in R, so I knew that I needed to switch to Python from the many solutions I found online.  I did just that, and it was a solid learning experience for me as I have been needing to enhance my Python skills, and this was a practical avenue to move forward with that objecive.

The following took a few weeks of research and there were a lot of late nights and weekends put into this effort.  In all, it was worth it, but I did quesiton pursuing this at times given the level of effort just to scrape these pages.  I really wanted to get the best lat lon information, and an earlier attempt to get coordinates using the addresses was not the best given that many of the addresses are missing information.  GVA seems to have better data on the coordinates behind the scenes.


```{python}

# run these in terminal
# pip install selenium
# pip install chromedriver
# pip install beautifulsoup4

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from bs4 import BeautifulSoup
from time import sleep
import pandas as pd

options = Options() # these are the options needed to hide that the browser was being controlled by a computer and not a human
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument("--profile-directory=Default")
options.add_experimental_option("useAutomationExtension", False)

driver = webdriver.Chrome(options=options) # this established the browser
first_page = "https://www.gunviolencearchive.org/incident/882422" # this is a default page to start with
driver.get(first_page) # this opens the URL in the established browser

# from here, I had to click on a second tab manually, and open another page on the same website which cleared the bot limiter I was running into

# then, I was able to run the loop as follows

url_list = r.geo_locations_incidents_tbl # creates a dataframe in python from the R dataframe created earlier
temp_list = {
    "geo_data": []
} # this is an empty list container that will hold the HTML text as we iterate over the pages.

for url in url_list["pages"]: # loop to open the pages, parse the HTML, parse, the text from the page, pop it into a list, rinse, and repeat
    driver.get(url)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    geo_loc = soup.get_text()
    temp_list["geo_data"].append(geo_loc)
    
df = pd.DataFrame.from_dict(temp_list) # this moves the list into a dataframe

driver.close() # this closes the session.

# note that I was not able to iterate of 4000 pages as the bot detector blocked my IPs from my VPN a few times
# I broke the chunks down into 200 to 300 at a time, and that seemed to do the trick
# I probably could have used a sleep command to slow the roll, which might have appeased the bot police....

```

```{r}
geo_data <- py$df # this command moves a Python object into an R object...Reticulate is pretty sweet!

write_rds(geo_data, "scrapped_geo_data.rds") # I stored the data as I went in case R crashed
```


```{r, include=TRUE, eval=FALSE}
geo_data <- read_rds("scrapped_geo_data.rds")

output <- geo_data %>% 
    transmute(test = str_extract(geo_data, "Geolocation.*")) # pull the geo location data

ms_tbl_updated <- ms_tbl %>% 
    bind_cols(output) # combine the original data with the new geo location data

tbl_to_update <- read_rds("ms_tbl_updated_geo_locs.rds") # use this to source a previously stored table and append it

ms_tbl_updated <- tbl_to_update %>% 
    bind_rows(ms_tbl_updated) # append the new data rows to the old

write_rds(ms_tbl_updated, "ms_tbl_updated_geo_locs.rds") # save it

ms_tbl_updated %>% 
    summarize(n = n(),
              n_dist = n_distinct(incident_id)) # check it
```

# Mapping

```{r map, include=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
ms_tbl_map_data <- read_rds("ms_tbl_updated_geo_locs.rds")

ms_tbl_map_prep <- ms_tbl_map_data %>% 
    rename("GeoLocation" = test) %>% 
    mutate(temp = GeoLocation,
           temp = str_remove_all(temp, "Geolocation: ")) %>% 
    separate(temp, into = c("latitude","longitude"), sep = ", ") %>% 
    mutate(across(latitude:longitude, ~ as.numeric(.)))

# List the available map tiles
#names(providers)

# Create a leaflet map with default map tile using addTiles()
ms_tbl_map_prep %>% 
    leaflet() %>%
    addProviderTiles("CartoDB") %>%
    addCircleMarkers(lng = longitude, lat = latitude,
                     label = lapply(str_glue("{month(ms_tbl_map_prep$incident_date, label = TRUE, abbr = TRUE)} {year(ms_tbl_map_prep$incident_date)} <br/> {ms_tbl_map_prep$city_or_county}, {ms_tbl_map_prep$state} <br/> {ms_tbl_map_prep$number_killed} killed | {ms_tbl_map_prep$number_injured} wounded"),
                                    htmltools::HTML), radius = 3, color = "steelblue", clusterOptions = markerClusterOptions()) %>%
    addProviderTiles("CartoDB", group = "CartoDB") %>%
    addProviderTiles("Esri", group = "Esri") %>%
    addLayersControl(baseGroups = c("Esri","CartoDB")) # Use addLayersControl to allow users to toggle between basemaps
```



```{r get geoloc, eval=FALSE}
# Incident Map
# get a list of unique cities and states;
# this reduces our list down to ~1090 vs ~3840, which will take about 18 minutes to geocode vs 50 minutes

unique_city_state <- ms_tbl %>%
    mutate(city_or_county = str_remove_all(city_or_county, "\\(.*\\)")) %>%
    distinct(city_or_county, state)

ms_lat_longs_nest <- unique_city_state %>%
    mutate(group_state = state) %>%
    group_by(group_state) %>%
    nest(data = -group_state) %>%
    mutate(mapper = map(.x = data, .f = function(data) { data %>%
                                                             geocode(city = city_or_county,
                                                         state = state,
                                                         method = 'osm',
                                                         lat = latitude ,
                                                         long = longitude) }))

ms_lat_longs <- ms_lat_longs_nest %>%
    unnest(mapper) %>%
    ungroup() %>%
    select(-group_state, -data) %>%
    inner_join(ms_tbl %>%
                   mutate(city_or_county = str_remove_all(city_or_county, "\\(.*\\)")),
               by = c("city_or_county","state"))

#save it so we don't have to re-run this every session

write_rds(x = ms_lat_longs, "ms_lat_longs.rds")
```

```{r load map data, include=TRUE, eval=TRUE}
ms_lat_longs <- read_rds("ms_lat_longs.rds")
```

```{r simple map, include=TRUE, eval=TRUE, message=FALSE, warning=FALSE, out.width="100%", out.height="100%"}
library(gganimate)

ms_tbl_map_prep %>% 
    mutate(year = factor(year)) %>% 
    filter(state != "Alaska") %>% 
    ggplot(aes(longitude, latitude, 
               size = number_killed + number_injured,
               color = year)) +
    geom_point() +
    borders("state") +
    scale_color_tq() +
    coord_map() +
    theme_light() +
    labs(title = "Map of Mass Shooting Incidents in {closest_state}",
         subtitle = "Size represents the total deaths and injuries per incident",
         x = "",
         y = "") +
    theme(legend.position = "none") +
    transition_states(year, transition_length = 1, state_length = 1) +
    enter_fade() +
    exit_fade()
```

